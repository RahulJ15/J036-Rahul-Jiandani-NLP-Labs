{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gMp3WtoiyWUT"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dqj1cE1qylhT",
    "outputId": "36d0b54b-2b9f-46a2-c6f9-f6c7dd4ad6cc"
   },
   "outputs": [],
   "source": [
    "model = api.load('word2vec-google-news-300') #loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zuoyp4LPy0Hs",
    "outputId": "2c48d926-5003-4b6e-bcff-e7d3e164787e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words for 'mobile':\n",
      "  mobile_phones (0.7054648399353027)\n",
      "  Mobile (0.6691668629646301)\n",
      "  smartphone (0.6600653529167175)\n",
      "  smartphones (0.6404396891593933)\n",
      "  handsets (0.6404278874397278)\n",
      "\n",
      "Similar words for 'dog':\n",
      "  dogs (0.8680490851402283)\n",
      "  puppy (0.8106428384780884)\n",
      "  pit_bull (0.780396044254303)\n",
      "  pooch (0.762737512588501)\n",
      "  cat (0.7609456777572632)\n",
      "\n",
      "Similar words for 'king':\n",
      "  kings (0.7138045430183411)\n",
      "  queen (0.6510956287384033)\n",
      "  monarch (0.6413194537162781)\n",
      "  crown_prince (0.6204219460487366)\n",
      "  prince (0.6159993410110474)\n",
      "\n",
      "Similar words for 'water':\n",
      "  potable_water (0.6799107193946838)\n",
      "  Water (0.670687198638916)\n",
      "  sewage (0.6619378328323364)\n",
      "  groundwater (0.65883469581604)\n",
      "  Floridan_aquifer (0.6422534584999084)\n",
      "\n",
      "Similar words for 'fire':\n",
      "  blaze (0.7516718506813049)\n",
      "  fires (0.7222490906715393)\n",
      "  Fire (0.69910728931427)\n",
      "  flames (0.638767421245575)\n",
      "  carelessly_discarded_cigarette (0.6215550303459167)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of words to find similar words for\n",
    "words = ['mobile','dog','king','water','fire']\n",
    "\n",
    "for word in words:\n",
    "    similar_words = model.most_similar(word, topn=5)\n",
    "    print(f\"Similar words for '{word}':\")\n",
    "    for similar_word, similarity in similar_words:\n",
    "        print(f\"  {similar_word} ({similarity})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqGyw6vT1NVV",
    "outputId": "9e6b78b1-9a19-4e98-8016-ecbba00df115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman ≈ queen (0.7118192911148071)\n",
      "Delhi - India + France ≈ Paris (0.6828771233558655)\n",
      "apple - fruit + vegetable ≈ potato (0.5865278244018555)\n",
      "doctor - hospital + school ≈ guidance_counselor (0.5969594717025757)\n",
      "bird - fly + swim ≈ swimming (0.557131826877594)\n"
     ]
    }
   ],
   "source": [
    "# Test analogies: most_similar method used to perform the analogy tests by specifying positive and negative words.\n",
    "analogies = [\n",
    "    ('king', 'man', 'woman'),\n",
    "    ('Delhi', 'India', 'France'),\n",
    "    ('apple', 'fruit', 'vegetable'),\n",
    "    #checks if the model can find a word related to \"vegetable\" in a similar way \"apple\" is related to \"fruit\"\n",
    "    ('doctor', 'hospital', 'school'),\n",
    "    ('bird', 'fly', 'swim')\n",
    "   \n",
    "]\n",
    "\n",
    "# analogy tests\n",
    "for analogy in analogies:\n",
    "    result = model.most_similar(positive=[analogy[0], analogy[2]], negative=[analogy[1]], topn=1)\n",
    "    print(f\"{analogy[0]} - {analogy[1]} + {analogy[2]} ≈ {result[0][0]} ({result[0][1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztKAomtU2iB0"
   },
   "source": [
    "# IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rGJhiv_J2cGj",
    "outputId": "d29056b7-1230-4f83-bc7f-65bf3e2dc750"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BbqGb6387I4f",
    "outputId": "6b9ea494-c39c-4973-ade6-65d828ddb281"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"D:\\IMDB Dataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "btULiLy86pxq",
    "outputId": "33a0c1b8-7089-4100-f866-8e752de1af0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive    25000\n",
      "negative    25000\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rcauj5NE7H5Q"
   },
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Remove punctuation and numbers\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    tokens = word_tokenize(text) # Tokenize the text\n",
    "    cleaned_tokens = [word for word in tokens if word not in stopwords.words('english')] # Remove stopwords\n",
    "    cleaned_text = ' '.join(cleaned_tokens) # Join tokens back to string\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_7LVkcE99Twv"
   },
   "outputs": [],
   "source": [
    "subset_df = df.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OPAwXE79schx"
   },
   "outputs": [],
   "source": [
    "subset_df['cleaned_review'] = subset_df['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0_MCtn29fkR",
    "outputId": "2ebb5a44-19e0-470d-b330-7aa739f0e50f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of the cleaned subset dataset:\n",
      "                                                  review  \\\n",
      "33553  I really liked this Summerslam due to the look...   \n",
      "9427   Not many television shows appeal to quite as m...   \n",
      "199    The film quickly gets to a major chase scene w...   \n",
      "12447  Jane Austen would definitely approve of this o...   \n",
      "39489  Expectations were somewhat high for me when I ...   \n",
      "\n",
      "                                          cleaned_review  \n",
      "33553  really liked summerslam due look arena curtain...  \n",
      "9427   many television shows appeal quite many differ...  \n",
      "199    film quickly gets major chase scene ever incre...  \n",
      "12447  jane austen would definitely approve onebr br ...  \n",
      "39489  expectations somewhat high went see movie thou...  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst few rows of the cleaned subset dataset:\")\n",
    "print(subset_df[['review', 'cleaned_review']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pdx8wkMqR1c"
   },
   "source": [
    "# Custom Skip-gram Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dyyl-7tqVIG",
    "outputId": "c68e631e-737b-4e55-d847-575dce46b26b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.52      0.53       506\n",
      "           1       0.53      0.56      0.55       494\n",
      "\n",
      "    accuracy                           0.54      1000\n",
      "   macro avg       0.54      0.54      0.54      1000\n",
      "weighted avg       0.54      0.54      0.54      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "# Subset the data\n",
    "subset_df = df.sample(frac=0.1, random_state=42)\n",
    "subset_df['cleaned_review'] = subset_df['review'].apply(clean_text)\n",
    "\n",
    "# Train custom Skip-gram model\n",
    "skipgram_model = Word2Vec(subset_df['cleaned_review'], vector_size=100, window=5, sg=1, min_count=1, workers=4)\n",
    "\n",
    "# Function to create document vectors by averaging word vectors\n",
    "def get_avg_word2vecs(model, tokens_list):\n",
    "    vector_size = model.wv.vector_size\n",
    "    doc_vectors = []\n",
    "    for tokens in tokens_list:\n",
    "        vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "        if vectors:\n",
    "            avg_vector = np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            avg_vector = np.zeros(vector_size)\n",
    "        doc_vectors.append(avg_vector)\n",
    "    return np.array(doc_vectors)\n",
    "\n",
    "# Create document vectors\n",
    "X_skipgram = get_avg_word2vecs(skipgram_model, subset_df['cleaned_review'])\n",
    "\n",
    "# Labels\n",
    "y = subset_df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# Split data\n",
    "X_train_sg, X_test_sg, y_train_sg, y_test_sg = train_test_split(X_skipgram, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate model\n",
    "rf_sg = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_sg.fit(X_train_sg, y_train_sg)\n",
    "y_pred_sg = rf_sg.predict(X_test_sg)\n",
    "print(\"Skip-gram Model Performance:\")\n",
    "print(classification_report(y_test_sg, y_pred_sg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dlt5ZMwwqZ8Q"
   },
   "source": [
    "# Custom CBoW Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cukyWbIfqYKR",
    "outputId": "ce051c24-5952-40fc-a20d-f9b7de9be1b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "Exception ignored in: <function SeekableUnicodeStreamReader.__del__ at 0x7f4f27b468c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nltk/data.py\", line 1160, in __del__\n",
      "    if not self.closed:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nltk/data.py\", line 1180, in closed\n",
      "    return self.stream.closed\n",
      "AttributeError: 'SeekableUnicodeStreamReader' object has no attribute 'stream'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBoW Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.54      0.56       506\n",
      "           1       0.56      0.59      0.57       494\n",
      "\n",
      "    accuracy                           0.57      1000\n",
      "   macro avg       0.57      0.57      0.57      1000\n",
      "weighted avg       0.57      0.57      0.57      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subset the data\n",
    "subset_df = df.sample(frac=0.1, random_state=42)\n",
    "subset_df['cleaned_review'] = subset_df['review'].apply(clean_text)\n",
    "\n",
    "# Train custom CBoW model\n",
    "cbow_model = Word2Vec(subset_df['cleaned_review'], vector_size=100, window=5, sg=0, min_count=1, workers=4)\n",
    "\n",
    "# Create document vectors\n",
    "X_cbow = get_avg_word2vecs(cbow_model, subset_df['cleaned_review'])\n",
    "\n",
    "# Labels\n",
    "y = subset_df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# Split data\n",
    "X_train_cb, X_test_cb, y_train_cb, y_test_cb = train_test_split(X_cbow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate model\n",
    "rf_cb = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_cb.fit(X_train_cb, y_train_cb)\n",
    "y_pred_cb = rf_cb.predict(X_test_cb)\n",
    "print(\"CBoW Model Performance:\")\n",
    "print(classification_report(y_test_cb, y_pred_cb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqfGtLOtqdqJ"
   },
   "source": [
    "# Word2Vec SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPqWYbr9qd4l",
    "outputId": "60da12f6-80c4-4b60-dc07-d6f6ce7ae77b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Word2Vec Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.56      0.58       506\n",
      "           1       0.57      0.60      0.59       494\n",
      "\n",
      "    accuracy                           0.58      1000\n",
      "   macro avg       0.58      0.58      0.58      1000\n",
      "weighted avg       0.58      0.58      0.58      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Subset the data\n",
    "subset_df = df.sample(frac=0.1, random_state=42)\n",
    "subset_df['cleaned_review'] = subset_df['review'].apply(clean_text)\n",
    "\n",
    "# Load the pretrained word2vec model\n",
    "pretrained_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Function to create document vectors using pretrained word2vec model\n",
    "def get_avg_pretrained_word2vecs(model, tokens_list):\n",
    "    vector_size = model.vector_size\n",
    "    doc_vectors = []\n",
    "    for tokens in tokens_list:\n",
    "        vectors = [model[token] for token in tokens if token in model]\n",
    "        if vectors:\n",
    "            avg_vector = np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            avg_vector = np.zeros(vector_size)\n",
    "        doc_vectors.append(avg_vector)\n",
    "    return np.array(doc_vectors)\n",
    "\n",
    "# Create document vectors\n",
    "X_pretrained = get_avg_pretrained_word2vecs(pretrained_model, subset_df['cleaned_review'])\n",
    "\n",
    "# Labels\n",
    "y = subset_df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# Split data\n",
    "X_train_pt, X_test_pt, y_train_pt, y_test_pt = train_test_split(X_pretrained, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate model\n",
    "rf_pt = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_pt.fit(X_train_pt, y_train_pt)\n",
    "y_pred_pt = rf_pt.predict(X_test_pt)\n",
    "print(\"Pretrained Word2Vec Model Performance:\")\n",
    "print(classification_report(y_test_pt, y_pred_pt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlH792hHq1yP"
   },
   "source": [
    "# Skip-gram Vectors with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKN3naSoqp2f",
    "outputId": "ab33135a-4e39-4fc2-cfda-861f66b65346"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram Model Performance with {'vector_size': 100, 'window': 5, 'min_count': 1}:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.53      0.54       506\n",
      "           1       0.54      0.58      0.56       494\n",
      "\n",
      "    accuracy                           0.55      1000\n",
      "   macro avg       0.55      0.55      0.55      1000\n",
      "weighted avg       0.55      0.55      0.55      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram Model Performance with {'vector_size': 150, 'window': 10, 'min_count': 2}:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.56      0.57       506\n",
      "           1       0.57      0.60      0.59       494\n",
      "\n",
      "    accuracy                           0.58      1000\n",
      "   macro avg       0.58      0.58      0.58      1000\n",
      "weighted avg       0.58      0.58      0.58      1000\n",
      "\n",
      "Skip-gram Model Performance with {'vector_size': 200, 'window': 5, 'min_count': 1}:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.54      0.55       506\n",
      "           1       0.54      0.56      0.55       494\n",
      "\n",
      "    accuracy                           0.55      1000\n",
      "   macro avg       0.55      0.55      0.55      1000\n",
      "weighted avg       0.55      0.55      0.55      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Subset the data\n",
    "subset_df = df.sample(frac=0.1, random_state=42)\n",
    "subset_df['cleaned_review'] = subset_df['review'].apply(clean_text)\n",
    "\n",
    "# Function to create document vectors by averaging word vectors\n",
    "def get_avg_word2vecs(model, tokens_list):\n",
    "    vector_size = model.wv.vector_size\n",
    "    doc_vectors = []\n",
    "    for tokens in tokens_list:\n",
    "        vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "        if vectors:\n",
    "            avg_vector = np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            avg_vector = np.zeros(vector_size)\n",
    "        doc_vectors.append(avg_vector)\n",
    "    return np.array(doc_vectors)\n",
    "\n",
    "# Labels\n",
    "y = subset_df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# Experiment with different hyperparameters for Skip-gram model\n",
    "hyperparameters = [\n",
    "    {'vector_size': 100, 'window': 5, 'min_count': 1},\n",
    "    {'vector_size': 150, 'window': 10, 'min_count': 2},\n",
    "    {'vector_size': 200, 'window': 5, 'min_count': 1},\n",
    "]\n",
    "\n",
    "for params in hyperparameters:\n",
    "    skipgram_model = Word2Vec(subset_df['cleaned_review'], vector_size=params['vector_size'], window=params['window'], sg=1, min_count=params['min_count'], workers=4)\n",
    "    X_skipgram = get_avg_word2vecs(skipgram_model, subset_df['cleaned_review'])\n",
    "\n",
    "    X_train_sg, X_test_sg, y_train_sg, y_test_sg = train_test_split(X_skipgram, y, test_size=0.2, random_state=42)\n",
    "    rf_sg = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_sg.fit(X_train_sg, y_train_sg)\n",
    "    y_pred_sg = rf_sg.predict(X_test_sg)\n",
    "    print(f\"Skip-gram Model Performance with {params}:\")\n",
    "    print(classification_report(y_test_sg, y_pred_sg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_gFdZI0q6Dg"
   },
   "source": [
    "# CBoW Vectors with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0h9nJe8Bq8SJ",
    "outputId": "eca295f2-d594-4823-b737-6235d41bc8a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBoW Model Performance with {'vector_size': 100, 'window': 5, 'min_count': 1}:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.56      0.56       506\n",
      "           1       0.56      0.58      0.57       494\n",
      "\n",
      "    accuracy                           0.57      1000\n",
      "   macro avg       0.57      0.57      0.57      1000\n",
      "weighted avg       0.57      0.57      0.57      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBoW Model Performance with {'vector_size': 150, 'window': 10, 'min_count': 2}:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.53      0.55       506\n",
      "           1       0.54      0.57      0.55       494\n",
      "\n",
      "    accuracy                           0.55      1000\n",
      "   macro avg       0.55      0.55      0.55      1000\n",
      "weighted avg       0.55      0.55      0.55      1000\n",
      "\n",
      "CBoW Model Performance with {'vector_size': 200, 'window': 5, 'min_count': 1}:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.55      0.56       506\n",
      "           1       0.55      0.57      0.56       494\n",
      "\n",
      "    accuracy                           0.56      1000\n",
      "   macro avg       0.56      0.56      0.56      1000\n",
      "weighted avg       0.56      0.56      0.56      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Subset the data\n",
    "subset_df = df.sample(frac=0.1, random_state=42)\n",
    "subset_df['cleaned_review'] = subset_df['review'].apply(clean_text)\n",
    "\n",
    "# Function to create document vectors by averaging word vectors\n",
    "def get_avg_word2vecs(model, tokens_list):\n",
    "    vector_size = model.wv.vector_size\n",
    "    doc_vectors = []\n",
    "    for tokens in tokens_list:\n",
    "        vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "        if vectors:\n",
    "            avg_vector = np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            avg_vector = np.zeros(vector_size)\n",
    "        doc_vectors.append(avg_vector)\n",
    "    return np.array(doc_vectors)\n",
    "\n",
    "# Labels\n",
    "y = subset_df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# Experiment with different hyperparameters for CBoW model\n",
    "hyperparameters = [\n",
    "    {'vector_size': 100, 'window': 5, 'min_count': 1},\n",
    "    {'vector_size': 150, 'window': 10, 'min_count': 2},\n",
    "    {'vector_size': 200, 'window': 5, 'min_count': 1},\n",
    "]\n",
    "\n",
    "for params in hyperparameters:\n",
    "    cbow_model = Word2Vec(subset_df['cleaned_review'], vector_size=params['vector_size'], window=params['window'], sg=0, min_count=params['min_count'], workers=4)\n",
    "    X_cbow = get_avg_word2vecs(cbow_model, subset_df['cleaned_review'])\n",
    "\n",
    "    X_train_cb, X_test_cb, y_train_cb, y_test_cb = train_test_split(X_cbow, y, test_size=0.2, random_state=42)\n",
    "    rf_cb = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_cb.fit(X_train_cb, y_train_cb)\n",
    "    y_pred_cb = rf_cb.predict(X_test_cb)\n",
    "    print(f\"CBoW Model Performance with {params}:\")\n",
    "    print(classification_report(y_test_cb, y_pred_cb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9NR4i12rG6w"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
